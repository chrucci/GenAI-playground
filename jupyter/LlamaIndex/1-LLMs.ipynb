cells:
  - cell_type: code
    execution_count: 2
    metadata: {}
    outputs: []
    source:
      - |
        import os
      - |
        import openai
      - |
        import sys
      - |+
      - |
        sys.path.append("../..")
      - |+
      - |
        from dotenv import load_dotenv, find_dotenv
      - |+
      - |
        _ = load_dotenv(find_dotenv())  # read local .env file
      - |+
      - openai.api_key = os.environ["OPENAI_API_KEY"]
  - cell_type: markdown
    metadata: {}
    source:
      - |
        ## Setup
      - |+
      - |
        - Load a test document
      - |
        - Create an LLM
      - '- Copy Prompts from Llama index to test with'
  - cell_type: code
    execution_count: 5
    metadata: {}
    outputs: []
    source:
      - |
        with open("./starter_example.md", "r") as f:
      - '    text = f.read()'
  - cell_type: code
    execution_count: 6
    metadata: {}
    outputs: []
    source:
      - |
        from llama_index.llms import OpenAI
      - |+
      - llm = OpenAI(temperature=0.0)
  - cell_type: code
    execution_count: 7
    metadata: {}
    outputs: []
    source:
      - |
        from llama_index import Prompt
      - |+
      - |
        text_qa_template = Prompt(
      - |2
            "Context information is below.\n"
      - |2
            "---------------------\n"
      - |2
            "{context_str}\n"
      - |2
            "---------------------\n"
      - |2
            "Given the context information and not prior knowledge, "
      - |2
            "answer the question: {query_str}\n"
      - |
        )
      - |+
      - |
        refine_template = Prompt(
      - |2
            "We have the opportunity to refine the original answer "
      - |2
            "(only if needed) with some more context below.\n"
      - |2
            "------------\n"
      - |2
            "{context_msg}\n"
      - |2
            "------------\n"
      - |2
            "Given the new context, refine the original answer to better "
      - |2
            "answer the question: {query_str}. "
      - |2
            "If the context isn't useful, output the original answer again.\n"
      - |2
            "Original Answer: {existing_answer}"
      - )
  - cell_type: markdown
    metadata: {}
    source:
      - '## Text QA Template Testing'
  - cell_type: code
    execution_count: 8
    metadata: {}
    outputs:
      - name: stdout
        output_type: stream
        text:
          - |
            To install LlamaIndex, you need to follow the installation steps provided in the "installation" guide. The specific steps are not mentioned in the given context information.
    source:
      - |
        question = "How can I install llama-index?"
      - |
        prompt = text_qa_template.format(context_str=text, query_str=question)
      - |
        response = llm.complete(prompt)
      - print(response.text)
  - cell_type: code
    execution_count: 9
    metadata: {}
    outputs:
      - name: stdout
        output_type: stream
        text:
          - |
            To create an index, you need to follow these steps:
          - |+
          - |
            1. Download the data you want to index and save it in a folder called `data`.
          - |
            2. Set your OpenAI API key as an environment variable.
          - |
            3. Create a Python file called `starter.py` in the same folder where you created the `data` folder.
          - |
            4. In `starter.py`, import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader` from `llama_index`.
          - |
            5. Load the data using `SimpleDirectoryReader("data").load_data()` and assign it to the `documents` variable.
          - |
            6. Build the index using `VectorStoreIndex.from_documents(documents)` and assign it to the `index` variable.
          - |
            7. Add the necessary lines to query your data using the index. For example, you can create a query engine using `index.as_query_engine()` and then query the engine with a question using `query_engine.query("Your question here")`.
          - |
            8. Optionally, you can add logging to see what's happening under the hood by adding the logging lines at the top of `starter.py`.
          - |
            9. Optionally, you can store the index to disk for later use by adding the line `index.storage_context.persist()`.
          - |
            10. Run the `starter.py` file to create and use the index.
          - |+
          - |
            Note: This is a simplified explanation. For more details and customization options, you can refer to the provided documentation.
    source:
      - |
        question = "How do I create an index?"
      - |
        prompt = text_qa_template.format(context_str=text, query_str=question)
      - |
        response = llm.complete(prompt)
      - print(response.text)
  - cell_type: code
    execution_count: 10
    metadata: {}
    outputs:
      - name: stdout
        output_type: stream
        text:
          - |
            ```python
          - |
            from llama_index import VectorStoreIndex, SimpleDirectoryReader
          - |+
          - |
            documents = SimpleDirectoryReader("data").load_data()
          - |
            index = VectorStoreIndex.from_documents(documents)
          - '```'
    source:
      - |
        # NOTE: the use of "stream_complete"
      - |+
      - |
        question = "How do I create an index? Write your answer using only code."
      - |
        prompt = text_qa_template.format(context_str=text, query_str=question)
      - |
        response_gen = llm.stream_complete(prompt)
      - |
        for response in response_gen:
      - '    print(response.delta, end="")'
  - cell_type: markdown
    metadata: {}
    source:
      - '## Refine Template Testing'
  - cell_type: code
    execution_count: 11
    metadata: {}
    outputs:
      - name: stdout
        output_type: stream
        text:
          - |
            To create an index using LlamaIndex, follow these steps:
          - |+
          - |
            1. Download the data file from the provided link and save it in a folder called `data`.
          - |
            2. Set your OpenAI API key as an environment variable.
          - |
            3. Create a Python file called `starter.py` in the same folder as the `data` folder.
          - |
            4. Import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.
          - |
            5. Load the documents from the `data` folder using `SimpleDirectoryReader("data").load_data()`.
          - |
            6. Build the index using `VectorStoreIndex.from_documents(documents)`.
          - |
            7. Query the index using the created engine and print the response.
          - |
            8. Add logging to see what's happening under the hood.
          - |
            9. Persist the index to disk using `index.storage_context.persist()`.
          - |
            10. Modify the code to load the index if it already exists, or create and store it if it doesn't.
          - |
            11. Query the index again and print the response.
          - |+
          - |
            Here's the refined code:
          - |+
          - |
            ```python
          - |
            import os.path
          - |
            import logging
          - |
            import sys
          - |
            from llama_index import (
          - |2
                VectorStoreIndex,
          - |2
                SimpleDirectoryReader,
          - |2
                StorageContext,
          - |2
                load_index_from_storage,
          - |
            )
          - |+
          - |
            # Set your OpenAI API key as an environment variable
          - |+
          - |
            # Load the documents and create the index
          - |
            documents = SimpleDirectoryReader("data").load_data()
          - |
            index = VectorStoreIndex.from_documents(documents)
          - |+
          - |
            # Query the index
          - |
            query_engine = index.as_query_engine()
          - |
            response = query_engine.query("What did the author do growing up?")
          - |
            print(response)
          - |+
          - |
            # Add logging
          - |
            logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
          - |
            logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
          - |+
          - |
            # Persist the index to disk
          - |
            index.storage_context.persist()
          - |+
          - |
            # Check if the index already exists
          - |
            if not os.path.exists("./storage"):
          - |2
                # Load the documents and create the index
          - |2
                documents = SimpleDirectoryReader("data").load_data()
          - |2
                index = VectorStoreIndex.from_documents(documents)
          - |2
                # Store it for later
          - |2
                index.storage_context.persist()
          - |
            else:
          - |2
                # Load the existing index
          - |2
                storage_context = StorageContext.from_defaults(persist_dir="./storage")
          - |2
                index = load_index_from_storage(storage_context)
          - |+
          - |
            # Query the index again
          - |
            query_engine = index.as_query_engine()
          - |
            response = query_engine.query("What did the author do growing up?")
          - |
            print(response)
          - |
            ```
          - |+
          - |
            This code will download the data, create an index, query the index, add logging, persist the index to disk, and load the index if it already exists.
    source:
      - |
        question = "How do I create an index? Write your answer using only code."
      - |
        existing_answer = """To create an index using LlamaIndex, you need to follow these steps:
      - |+
      - |
        1. Download the LlamaIndex repository by cloning it from GitHub.
      - |
        2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.
      - |
        3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.
      - |
        4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.
      - |
        5. Build the index using `VectorStoreIndex.from_documents(documents)`.
      - |
        6. To persist the index to disk, use `index.storage_context.persist()`.
      - |
        7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.
      - |+
      - |
        Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies."""
      - |
        prompt = refine_template.format(
      - |2
            context_msg=text, query_str=question, existing_answer=existing_answer
      - |
        )
      - |
        response = llm.complete(prompt)
      - print(response.text)
  - cell_type: markdown
    metadata: {}
    source:
      - |
        ## Chat Example
      - The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session.
  - cell_type: code
    execution_count: 12
    metadata: {}
    outputs:
      - name: stdout
        output_type: stream
        text:
          - |
            assistant: To create an index, you will need to follow these general steps:
          - |+
          - |
            1. Determine the purpose and scope of your index: Decide what information you want to include in your index and what it will be used for. This will help you determine the structure and content of your index.
          - |+
          - |
            2. Identify the items to be indexed: Determine the specific items or topics that you want to include in your index. For example, if you are creating an index for a book, you might want to index chapters, sections, and important concepts.
          - |+
          - |
            3. Create a list of index terms: Identify the key terms or phrases that will be used to reference each item in your index. These terms should be concise and descriptive.
          - |+
          - |
            4. Organize the index terms: Determine the hierarchical structure of your index. You can use headings, subheadings, and indentation to create a logical and organized structure.
          - |+
          - |
            5. Assign page numbers or locations: For each index term, identify the page number or location where the item can be found. This will help users quickly locate the information they are looking for.
          - |+
          - |
            6. Format the index: Use a consistent and clear formatting style for your index. You can use software tools like Microsoft Word or Adobe InDesign to create a professional-looking index.
          - |+
          - |
            7. Review and revise: Once you have created your index, review it carefully to ensure accuracy and completeness. Make any necessary revisions or updates before finalizing your index.
          - |+
          - |
            Remember, creating an index can be a time-consuming process, so it's important to plan and allocate enough time to complete it accurately.
    source:
      - |
        from llama_index.llms import ChatMessage
      - |+
      - |
        chat_history = [
      - |2
            ChatMessage(
      - |2
                role="system",
      - |2
                content="You are a helpful QA chatbot that can answer questions about llama-index.",
      - |2
            ),
      - |2
            ChatMessage(role="user", content="How do I create an index?"),
      - |
        ]
      - |+
      - |
        response = llm.chat(chat_history)
      - print(response.message)
  - cell_type: code
    execution_count: null
    metadata: {}
    outputs: []
    source: []
metadata:
  kernelspec:
    display_name: llamaindex
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.11.5
nbformat: 4
nbformat_minor: 2