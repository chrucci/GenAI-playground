{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Load a test document\n",
    "- Create an LLM\n",
    "- Copy Prompts from Llama index to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./starter_example.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Prompt\n",
    "\n",
    "text_qa_template = Prompt(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_template = Prompt(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text QA Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you need to follow the installation steps provided in the \"installation\" guide. The specific steps are not mentioned in the given context information.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I install llama-index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index, you need to follow these steps:\n",
      "\n",
      "1. Download the data you want to index and save it in a folder called `data`.\n",
      "2. Set your OpenAI API key as an environment variable.\n",
      "3. Create a Python file called `starter.py` in the same folder where you created the `data` folder.\n",
      "4. In `starter.py`, import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader` from `llama_index`.\n",
      "5. Load the data using `SimpleDirectoryReader(\"data\").load_data()` and assign it to the `documents` variable.\n",
      "6. Build the index using `VectorStoreIndex.from_documents(documents)` and assign it to the `index` variable.\n",
      "7. Add the necessary lines to query your data using the index. For example, you can create a query engine using `index.as_query_engine()` and then query the engine with a question using `query_engine.query(\"Your question here\")`.\n",
      "8. Optionally, you can add logging to see what's happening under the hood by adding the logging lines at the top of `starter.py`.\n",
      "9. Optionally, you can store the index to disk for later use by adding the line `index.storage_context.persist()`.\n",
      "10. Run the `starter.py` file to create and use the index.\n",
      "\n",
      "Note: This is a simplified explanation. For more details and customization options, you can refer to the provided documentation.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader(\"data\").load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```"
     ]
    }
   ],
   "source": [
    "# NOTE: the use of \"stream_complete\"\n",
    "\n",
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, follow these steps:\n",
      "\n",
      "1. Download the data file from the provided link and save it in a folder called `data`.\n",
      "2. Set your OpenAI API key as an environment variable.\n",
      "3. Create a Python file called `starter.py` in the same folder as the `data` folder.\n",
      "4. Import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
      "5. Load the documents from the `data` folder using `SimpleDirectoryReader(\"data\").load_data()`.\n",
      "6. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
      "7. Query the index using the created engine and print the response.\n",
      "8. Add logging to see what's happening under the hood.\n",
      "9. Persist the index to disk using `index.storage_context.persist()`.\n",
      "10. Modify the code to load the index if it already exists, or create and store it if it doesn't.\n",
      "11. Query the index again and print the response.\n",
      "\n",
      "Here's the refined code:\n",
      "\n",
      "```python\n",
      "import os.path\n",
      "import logging\n",
      "import sys\n",
      "from llama_index import (\n",
      "    VectorStoreIndex,\n",
      "    SimpleDirectoryReader,\n",
      "    StorageContext,\n",
      "    load_index_from_storage,\n",
      ")\n",
      "\n",
      "# Set your OpenAI API key as an environment variable\n",
      "\n",
      "# Load the documents and create the index\n",
      "documents = SimpleDirectoryReader(\"data\").load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "# Query the index\n",
      "query_engine = index.as_query_engine()\n",
      "response = query_engine.query(\"What did the author do growing up?\")\n",
      "print(response)\n",
      "\n",
      "# Add logging\n",
      "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
      "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
      "\n",
      "# Persist the index to disk\n",
      "index.storage_context.persist()\n",
      "\n",
      "# Check if the index already exists\n",
      "if not os.path.exists(\"./storage\"):\n",
      "    # Load the documents and create the index\n",
      "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
      "    index = VectorStoreIndex.from_documents(documents)\n",
      "    # Store it for later\n",
      "    index.storage_context.persist()\n",
      "else:\n",
      "    # Load the existing index\n",
      "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
      "    index = load_index_from_storage(storage_context)\n",
      "\n",
      "# Query the index again\n",
      "query_engine = index.as_query_engine()\n",
      "response = query_engine.query(\"What did the author do growing up?\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "This code will download the data, create an index, query the index, add logging, persist the index to disk, and load the index if it already exists.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Example\n",
    "The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: To create an index, you will need to follow these general steps:\n",
      "\n",
      "1. Determine the purpose and scope of your index: Decide what information you want to include in your index and what it will be used for. This will help you determine the structure and content of your index.\n",
      "\n",
      "2. Identify the items to be indexed: Determine the specific items or topics that you want to include in your index. For example, if you are creating an index for a book, you might want to index chapters, sections, and important concepts.\n",
      "\n",
      "3. Create a list of index terms: Identify the key terms or phrases that will be used to reference each item in your index. These terms should be concise and descriptive.\n",
      "\n",
      "4. Organize the index terms: Determine the hierarchical structure of your index. You can use headings, subheadings, and indentation to create a logical and organized structure.\n",
      "\n",
      "5. Assign page numbers or locations: For each index term, identify the page number or location where the item can be found. This will help users quickly locate the information they are looking for.\n",
      "\n",
      "6. Format the index: Use a consistent and clear formatting style for your index. You can use software tools like Microsoft Word or Adobe InDesign to create a professional-looking index.\n",
      "\n",
      "7. Review and revise: Once you have created your index, review it carefully to ensure accuracy and completeness. Make any necessary revisions or updates before finalizing your index.\n",
      "\n",
      "Remember, creating an index can be a time-consuming process, so it's important to plan and allocate enough time to complete it accurately.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
