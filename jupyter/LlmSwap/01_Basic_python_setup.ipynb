{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Open LLMs with the OpenAI API\n",
    "This Notebook uses the same basic OpenAI Python API to run different local and serverless models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (1.14.1)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.29 (from langchain)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.31-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.9.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/chris/anaconda3/envs/ollama/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.0-py3-none-any.whl (32 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading orjson-3.9.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langchain_openai-0.1.0 langsmith-0.1.31 orjson-3.9.15 packaging-23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load .env file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# # Load .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# Set model variables\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# OPENAI_ORGANIZATION = \"FOO\"\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "OLLAMA_API_KEY = \"ollama\"\n",
    "\n",
    "# TOGETHER_BASE_URL = \"https://api.together.xyz\"\n",
    "# TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tools and models for all demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that can access external functions. Please provide responses based on the information from these function calls.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the current temperature of New York, San Francisco and Chicago?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a serverless Mixtral with the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"call_aohq18yhi3yav8p7z81jyjhw\",\n",
      "    \"function\": {\n",
      "      \"arguments\": \"{\\\"location\\\":\\\"New York, NY\\\",\\\"format\\\":\\\"fahrenheit\\\"}\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"call_2wbb93smogfhtoiq5gcac2pw\",\n",
      "    \"function\": {\n",
      "      \"arguments\": \"{\\\"location\\\":\\\"San Francisco, CA\\\",\\\"format\\\":\\\"fahrenheit\\\"}\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"call_3zkezuh2293wzjjiqwn1g63d\",\n",
      "    \"function\": {\n",
      "      \"arguments\": \"{\\\"location\\\":\\\"Chicago, IL\\\",\\\"format\\\":\\\"fahrenheit\\\"}\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=TOGETHER_BASE_URL,\n",
    "    api_key=TOGETHER_API_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    messages=MESSAGES,\n",
    "    tools=TOOLS,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(json.dumps(response.choices[0].message.model_dump()[\"tool_calls\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can even stream responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\n",
      "  {\n",
      "    \"name\": \"get_current_weather\",\n",
      "    \"arguments\": {\n",
      "      \"location\": \"New York, NY\",\n",
      "      \"format\": \"fahrenheit\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"get_current_weather\",\n",
      "    \"arguments\": {\n",
      "      \"location\": \"San Francisco, CA\",\n",
      "      \"format\": \"fahrenheit\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"get_current_weather\",\n",
      "    \"arguments\": {\n",
      "      \"location\": \"Chicago, IL\",\n",
      "      \"format\": \"fahrenheit\"\n",
      "    }\n",
      "  }\n",
      "]"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    messages=MESSAGES,\n",
    "    stream=True,\n",
    "    tools=TOOLS,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5 different models with the same API: 2 Serverless, 2 Local, 1 OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"description\": \"Llama2 Locally\",\n",
    "        \"base_url\": OLLAMA_BASE_URL,\n",
    "        \"api_key\": OLLAMA_API_KEY,\n",
    "        \"model\": \"llama2\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Mistral Locally\",\n",
    "        \"base_url\": OLLAMA_BASE_URL,\n",
    "        \"api_key\": OLLAMA_API_KEY,\n",
    "        \"model\": \"mistral\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Llama2 Serverless\",\n",
    "        \"base_url\": TOGETHER_BASE_URL,\n",
    "        \"api_key\": TOGETHER_API_KEY,\n",
    "        \"model\": \"togethercomputer/llama-2-13b-chat\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Mixtral Serverless\",\n",
    "        \"base_url\": TOGETHER_BASE_URL,\n",
    "        \"api_key\": TOGETHER_API_KEY,\n",
    "        \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"OpenAI Serverless\",\n",
    "        \"base_url\": OPENAI_BASE_URL,\n",
    "        \"api_key\": OPENAI_API_KEY,\n",
    "        \"model\": \"gpt-3.5-turbo-1106\",\n",
    "    },\n",
    "]\n",
    "\n",
    "MESSAGES_FOR_POEMS = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a poet who writes whimsical haikus about a given topic.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Topic: Ducks\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Llama2 Locally:\n",
      "-----------------------------\n",
      "\n",
      "Quacking in the pond,\n",
      "Feathers fluffed, beaks dipped,\n",
      "Ducks swim and play.\n",
      "\n",
      "Ribbons of blue and gold,\n",
      "Flapping wings, a gentle sight,\n",
      "Ducks take flight.\n",
      "\n",
      "In the sun or rain,\n",
      "Quacking ducks bring joy again,\n",
      "Waterfowl delight.\n",
      "\n",
      "Splashes in the water,\n",
      "Feathers flung, a ducky dance,\n",
      "Happiness abounds.\n",
      "\n",
      "\n",
      "Running Mistral Locally:\n",
      "-----------------------------\n",
      "\n",
      " Quiet pond reflects,\n",
      "Beneath, ducks weave graceful patterns,\n",
      "Nature's art unfolds.\n",
      "\n",
      "Dancing in the rain,\n",
      "Ducks splash and play, pure joy abounds,\n",
      "Life's simple pleasures.\n",
      "\n",
      "Morning sun illumines,\n",
      "Feathered friends paddle calmly by,\n",
      "Serenity reigns.\n",
      "\n",
      "Beneath the autumn sky,\n",
      "Ducks align in perfect rows,\n",
      "Autumn's graceful grace.\n",
      "\n",
      "\n",
      "Running Llama2 Serverless:\n",
      "-----------------------------\n",
      "\n",
      " Quacking in the pond,\n",
      "Feathers fluffed and shining bright,\n",
      "Ducks dance in the sun.\n",
      "\n",
      "\n",
      "Running Mixtral Serverless:\n",
      "-----------------------------\n",
      "\n",
      "Feathers, bright and dappled,\n",
      "Water's surface, gently broken,\n",
      "Ducks, a dance of joy.\n",
      "\n",
      "In the pond, they play,\n",
      "Quacking tales of wind and rain,\n",
      "Ducks, in rhythm, sway.\n",
      "\n",
      "Sunset's glow, so soft,\n",
      "Ducks, in twilight, tuck away,\n",
      "Dreams of flight take hold.\n",
      "\n",
      "\n",
      "Running OpenAI Serverless:\n",
      "-----------------------------\n",
      "\n",
      "Majestic mallards\n",
      "Gliding on the tranquil pond\n",
      "Quacking with delight\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_CONFIGS:\n",
    "    print(f\"Running {model['description']}:\")\n",
    "    print(f\"-----------------------------\\n\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=model[\"base_url\"],\n",
    "        api_key=model[\"api_key\"],\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model[\"model\"],\n",
    "        messages=MESSAGES_FOR_POEMS,\n",
    "    )\n",
    "\n",
    "    print(f\"{response.choices[0].message.content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also works well with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Llama2 Locally:\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Quacking echoes flow\n",
      "Through the pond's reflective depths\n",
      "Ducks dance with grace\n",
      "\n",
      "\n",
      "Running Mistral Locally:\n",
      "-----------------------------\n",
      "\n",
      " Quilt of feathers swirl,\n",
      "River's mirror dances bright,\n",
      "Ducks paddle, twinkle, world.\n",
      "\n",
      "\n",
      "Running Llama2 Serverless:\n",
      "-----------------------------\n",
      "\n",
      " Sure! Here's a whimsical haiku about ducks:\n",
      "\n",
      "Quacking in the pond,\n",
      "Feathers fluffed and feisty,\n",
      "Ducks dance on water.\n",
      "\n",
      "\n",
      "Running Mixtral Serverless:\n",
      "-----------------------------\n",
      "\n",
      "Quacking in glee,\n",
      "Feathers of rainbow hues,\n",
      "Pond dance in delight.\n",
      "\n",
      "\n",
      "Running OpenAI Serverless:\n",
      "-----------------------------\n",
      "\n",
      "Feathers float and glide\n",
      "Quacking waddlers on the pond\n",
      "Ducks bring joy inside\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "for model in MODEL_CONFIGS:\n",
    "    print(f\"Running {model['description']}:\")\n",
    "    print(f\"-----------------------------\\n\")\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=model[\"base_url\"],\n",
    "        api_key=model[\"api_key\"],\n",
    "        model=model[\"model\"],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"Write a whimsical haiku about a given topic.\"),\n",
    "            (\"user\", \"Topic: Ducks\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    poem_runnable = prompt | llm\n",
    "\n",
    "    response = poem_runnable.invoke({})\n",
    "    print(f\"{response.content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
