{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10b768-f5b4-43bc-a785-99077422ce78",
   "metadata": {},
   "source": [
    "# Lesson 3: Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14",
   "metadata": {},
   "source": [
    "In this lesson, you will familiarize yourself with the chatbot example you will work on during this course. The example includes the tool definitions and execution, as well as the chatbot code. Make sure to interact with the chatbot at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd02aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv (from -r requirements.txt (line 1))\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anthropic (from -r requirements.txt (line 3))\n",
      "  Downloading anthropic-0.51.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv->-r requirements.txt (line 1))\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting requests~=2.32.0 (from arxiv->-r requirements.txt (line 1))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic->-r requirements.txt (line 3))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic->-r requirements.txt (line 3))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.25.0 (from anthropic->-r requirements.txt (line 3))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic->-r requirements.txt (line 3))\n",
      "  Using cached jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from anthropic->-r requirements.txt (line 3))\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting sniffio (from anthropic->-r requirements.txt (line 3))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.10 (from anthropic->-r requirements.txt (line 3))\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->anthropic->-r requirements.txt (line 3))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv->-r requirements.txt (line 1))\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Collecting certifi (from httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 3))\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 3))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->anthropic->-r requirements.txt (line 3))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 3))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 3))\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->anthropic->-r requirements.txt (line 3))\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests~=2.32.0->arxiv->-r requirements.txt (line 1))\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests~=2.32.0->arxiv->-r requirements.txt (line 1))\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading anthropic-0.51.0-py3-none-any.whl (263 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: sgmllib3k, urllib3, typing-extensions, sniffio, python-dotenv, jiter, idna, h11, feedparser, distro, charset-normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, httpcore, anyio, pydantic, httpx, arxiv, anthropic\n",
      "Successfully installed annotated-types-0.7.0 anthropic-0.51.0 anyio-4.9.0 arxiv-2.2.0 certifi-2025.4.26 charset-normalizer-3.4.2 distro-1.9.0 feedparser-6.0.11 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.9.0 pydantic-2.11.4 pydantic-core-2.33.2 python-dotenv-1.1.0 requests-2.32.3 sgmllib3k-1.0.0 sniffio-1.3.1 typing-extensions-4.13.2 typing-inspection-0.4.0 urllib3-2.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f163a-87af-4e0c-87ed-1624c150c572",
   "metadata": {},
   "source": [
    "## Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549a7f46-74b3-4a1d-b084-055c99e3c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43905e-56f3-404c-a322-f038055e9b1c",
   "metadata": {},
   "source": [
    "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the `papers` directory. The tool does not download the papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886633b8-ce67-4343-822d-cc3f98f953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "\n",
    "    # Use arxiv to find the papers\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query=topic, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "\n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"summary\": paper.summary,\n",
    "            \"pdf_url\": paper.pdf_url,\n",
    "            \"published\": str(paper.published.date()),\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "\n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "\n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "\n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/computers/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1310.7911v2',\n",
       " 'math/9711204v1',\n",
       " '2208.00733v1',\n",
       " '2504.07020v1',\n",
       " '2403.03925v1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"computers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb83565-69af-47f3-9ba3-a96965cff7df",
   "metadata": {},
   "source": [
    "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "\n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "\n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    "\n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Compact manifolds with computable boundaries\",\\n  \"authors\": [\\n    \"Zvonko Iljazovic\"\\n  ],\\n  \"summary\": \"We investigate conditions under which a co-computably enumerable closed set\\\\nin a computable metric space is computable and prove that in each locally\\\\ncomputable computable metric space each co-computably enumerable compact\\\\nmanifold with computable boundary is computable. In fact, we examine the notion\\\\nof a semi-computable compact set and we prove a more general result: in any\\\\ncomputable metric space each semi-computable compact manifold with computable\\\\nboundary is computable. In particular, each semi-computable compact\\\\n(boundaryless) manifold is computable.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1310.7911v2\",\\n  \"published\": \"2013-10-29\"\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info(\"1310.7911v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4",
   "metadata": {},
   "source": [
    "## Tool Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d2260-452d-472a-b56e-326479cb18c9",
   "metadata": {},
   "source": [
    "Here are the schema of each tool which you will provide to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\"type\": \"string\", \"description\": \"The topic to search for\"},\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5,\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"topic\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2",
   "metadata": {},
   "source": [
    "## Tool Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4",
   "metadata": {},
   "source": [
    "This code handles tool mapping and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90790c0-efc4-4068-9c00-d2592d80bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\"search_papers\": search_papers, \"extract_info\": extract_info}\n",
    "\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any results.\"\n",
    "\n",
    "    elif isinstance(result, list):\n",
    "        result = \", \".join(result)\n",
    "\n",
    "    elif isinstance(result, dict):\n",
    "        # Convert dictionaries to formatted JSON strings\n",
    "        result = json.dumps(result, indent=2)\n",
    "\n",
    "    else:\n",
    "        # For any other type, convert using str()\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31",
   "metadata": {},
   "source": [
    "## Chatbot Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087",
   "metadata": {},
   "source": [
    "The chatbot handles the user's queries one by one, but it does not persist memory across the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe662400-8506-464e-a3da-75a3d8848bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175586b4-acdf-4103-8039-134478a4f797",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a896e0-3f56-417e-aa51-c61756048593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        max_tokens=2024,\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        tools=tools,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "\n",
    "        for content in response.content:\n",
    "            if content.type == \"text\":\n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "\n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "\n",
    "            elif content.type == \"tool_use\":\n",
    "                assistant_content.append(content)\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "\n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"tool_result\",\n",
    "                                \"tool_use_id\": tool_id,\n",
    "                                \"content\": result,\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                response = client.messages.create(\n",
    "                    max_tokens=2024,\n",
    "                    model=\"claude-3-7-sonnet-20250219\",\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
    "                    print(response.content[0].text)\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9",
   "metadata": {},
   "source": [
    "### Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16979cdc-81e9-432b-ba7f-e810b52961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "\n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3",
   "metadata": {},
   "source": [
    "Feel free to interact with the chatbot. Here's an example query: \n",
    "\n",
    "- Search for 2 papers on \"LLM interpretability\"\n",
    "\n",
    "To access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Hello! I'm here to help you search for and extract information about academic papers on arXiv. \n",
      "\n",
      "I can:\n",
      "- Search for papers on specific topics\n",
      "- Extract detailed information about a particular paper using its ID\n",
      "\n",
      "What would you like to know about? You could ask me to search for papers on a specific topic like quantum computing, artificial intelligence, climate change, or any other research area you're interested in.\n",
      "\n",
      "\n",
      "Hello! I'm here to help you search for scientific papers on arXiv and extract information about specific papers. \n",
      "\n",
      "What topic would you like to search for, or is there a specific paper ID you'd like to know more about?\n",
      "\n",
      "\n",
      "I'd be happy to help you find information about navigation systems. This is a broad topic that can include GPS systems, autonomous vehicle navigation, indoor navigation, marine navigation, and more.\n",
      "\n",
      "To provide you with the most relevant results, I can search for papers on arXiv related to navigation systems. Let me do that for you:\n",
      "Calling tool search_papers with args {'topic': 'navigation systems', 'max_results': 5}\n",
      "Results are saved in: papers/navigation_systems/papers_info.json\n",
      "I've found 5 papers on navigation systems. Let me extract the detailed information for each of these papers so you can see what they're about:\n",
      "Calling tool extract_info with args {'paper_id': '2010.10104v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2504.15643v1'}\n",
      "Calling tool extract_info with args {'paper_id': '1708.05006v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2208.09635v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2103.04445v1'}\n",
      "Here's a summary of the navigation systems papers I found for you:\n",
      "\n",
      "1. **Polarized Skylight Navigation System (PSNS)**:\n",
      "   - A bioinspired navigation system that imitates how insects use polarized skylight\n",
      "   - Discusses integration with GNSS and SINS (strapdown inertial navigation system)\n",
      "   - Published in 2020\n",
      "\n",
      "2. **Multimodal Perception for Goal-oriented Navigation**:\n",
      "   - A comprehensive survey of navigation approaches using visual, linguistic, and acoustic information\n",
      "   - Analyzes how agents perceive, reason about, and navigate complex environments\n",
      "   - Published in 2025\n",
      "\n",
      "3. **A Survey of Augmented Reality Navigation**:\n",
      "   - Explores how augmented reality can improve navigation systems\n",
      "   - Reviews existing research and techniques for building AR navigation systems\n",
      "   - Published in 2017\n",
      "\n",
      "4. **Mobile Robot Navigation in Complex Polygonal Workspaces**:\n",
      "   - Proposes a \"conformal navigation transformation\" for collision-free robot navigation\n",
      "   - Focuses on navigating around polygonal obstacles\n",
      "   - Published in 2022\n",
      "\n",
      "5. **Correct-by-Construction Navigation Functions**:\n",
      "   - Combines navigation transformation and harmonic functions\n",
      "   - Enables robot navigation in partially or fully unknown environments\n",
      "   - Provides for online, fast-feedback based navigation\n",
      "   - Published in 2021\n",
      "\n",
      "Would you like me to search for more specific aspects of navigation systems or would you like more details about any of these particular papers?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7890-4b4c-4ec9-b06f-abc8c4a290e8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> 🚨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34ee2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> 💻 &nbsp; <b> To Access the <code>requirements.txt</code> file or the <code>papers</code> folder: </b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>\"L3\"</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508916f3-8fa1-4e21-bfa7-081a810bc36c",
   "metadata": {},
   "source": [
    "In the next lessons, you will take out the tool definitions to wrap them in an MCP server. Then you will create an MCP client inside the chatbot to make the chatbot MCP compatible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d207b-e07d-49ff-bb03-7954aa86c167",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Guide on how to implement tool use](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#how-to-implement-tool-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5135e-01c3-4632-9f83-a1e6dd811049",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
