{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': 'Tag the piece of text with particular info.',\n",
       " 'parameters': {'title': 'Tagging',\n",
       "  'description': 'Tag the piece of text with particular info.',\n",
       "  'type': 'object',\n",
       "  'properties': {'sentiment': {'title': 'Sentiment',\n",
       "    'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
       "    'type': 'string'},\n",
       "   'language': {'title': 'Language',\n",
       "    'description': 'language of text (should be ISO 639-1 code)',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "    \n",
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# force it to use the tagging function\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "\n",
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Tagging', 'arguments': '{\\n  \"sentiment\": \"pos\",\\n  \"language\": \"en\"\\n}'}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Tagging', 'arguments': '{\\n  \"sentiment\": \"neg\",\\n  \"language\": \"it\"\\n}'}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neg', 'language': 'it'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can use the JsonOutputFunctionsParser to get only the pertinent info\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "Extraction is similar to tagging, but used for extracting multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")\n",
    "    \n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Information', 'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 25\\n    },\\n    {\\n      \"name\": \"Dan\",\\n      \"age\": 27\\n    },\\n    {\\n      \"name\": \"Julie\",\\n      \"age\": 19\\n    }\\n  ]\\n}'}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_prompt_str = \"Joe and his best friend Dan just met Julie, who is 19.\"\n",
    "extraction_model.invoke(age_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Information', 'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 0\\n    },\\n    {\\n      \"name\": \"Dan\",\\n      \"age\": 0\\n    },\\n    {\\n      \"name\": \"Julie\",\\n      \"age\": 19\\n    }\\n  ]\\n}'}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because the model hallucinated, we should prompt it better\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info.  Do not default any optional values.  Zero should never be used for age unless listed explicitly as such in the text.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model\n",
    "extraction_chain.invoke({\"input\": age_prompt_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': 'Joe', 'age': 0},\n",
       "  {'name': 'Dan', 'age': 0},\n",
       "  {'name': 'Julie', 'age': 19}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "jsonParser = JsonOutputFunctionsParser()\n",
    "extraction_chain = prompt | extraction_model | jsonParser\n",
    "extraction_chain.invoke({\"input\": age_prompt_str})\n",
    "\n",
    "# jsonParser gets us only the output, but I don't want the \"people\" variable name.  I just want the list of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joe', 'age': 0},\n",
       " {'name': 'Dan', 'age': 0},\n",
       " {'name': 'Julie', 'age': 19}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, I'll use the KeyOutputparser\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "peopleKeyParser = JsonKeyOutputFunctionsParser(key_name=\"people\")\n",
    "extraction_chain = prompt | extraction_model | peopleKeyParser\n",
    "extraction_chain.invoke({\"input\": age_prompt_str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it on a large body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at the content\n",
    "doc = documents[0]\n",
    "page_content = doc.page_content[:10000]\n",
    "end_page_content = doc.page_content[-10000:]\n",
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")\n",
    "    \n",
    "overview_tagging_function = convert_pydantic_to_openai_function(Overview)\n",
    "\n",
    "tagging_model = model.bind(\n",
    "    functions=[overview_tagging_function],\n",
    "    function_call={\"name\":\"Overview\"}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\n",
    "output = tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's extract the references to papers that are listed in this article\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]\n",
    "    \n",
    "paper_extraction_function = convert_pydantic_to_openai_function(Info)\n",
    "\n",
    "extraction_model = model.bind(\n",
    "    functions=[paper_extraction_function], \n",
    "    function_call={\"name\":\"Info\"}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "extraction_chain.invoke({\"input\": page_content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al.'},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash'},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)',\n",
       "  'author': 'Laskin et al.'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting on splits\n",
    "This article is huge, though.  We want to do with extraction on the whole thing, not just the first part.\n",
    "so we'll need to:\n",
    "- split the article\n",
    "- Pass those splits the LLM one by one\n",
    "- Combine all the results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    # the matrix is composed of a list of lists.  this function will return a single flattened list with the elements of each list from the matrix\n",
    "    flattened = []\n",
    "    for sublist in matrix:\n",
    "        for item in sublist:\n",
    "            flattened.append(item)\n",
    "            \n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass each split to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the output of ```prep``` is a list, so that means we want to call the next step multiple times\n",
    "# to do that we call map()\n",
    "# then we call flatten() to flatten the list\n",
    "chain = prep | extraction_chain.map() | flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al.'},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash'},\n",
       " {'title': 'Reflexion framework', 'author': 'Shinn & Labash'},\n",
       " {'title': 'Chain of Hindsight', 'author': 'Liu et al.'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al.'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'ED (expert distillation)', 'author': ''},\n",
       " {'title': 'RL^2', 'author': 'Duan et al. 2017'},\n",
       " {'title': 'LSH: Locality-Sensitive Hashing', 'author': ''},\n",
       " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah', 'author': ''},\n",
       " {'title': 'HNSW: Hierarchical Navigable Small World', 'author': ''},\n",
       " {'title': 'FAISS: Facebook AI Similarity Search', 'author': ''},\n",
       " {'title': 'ScaNN: Scalable Nearest Neighbors', 'author': ''},\n",
       " {'title': 'MRKL: Modular Reasoning, Knowledge and Language',\n",
       "  'author': 'Karpas et al. 2022'},\n",
       " {'title': 'TALM: Tool Augmented Language Models',\n",
       "  'author': 'Parisi et al. 2022'},\n",
       " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
       " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
       " {'title': 'API-Bank: A Benchmark for Evaluating Tool-Augmented Language Models',\n",
       "  'author': 'Li et al. 2023'},\n",
       " {'title': 'ChemCrow: Augmenting Language Models with Expert-Designed Tools for Scientific Discovery',\n",
       "  'author': 'Bran et al. 2023'},\n",
       " {'title': 'Boiko et al. (2023)', 'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
       " {'title': 'Park et al. 2023', 'author': ''},\n",
       " {'title': 'Super Mario: How Nintendo Conquered America',\n",
       "  'author': 'Jeff Ryan'},\n",
       " {'title': 'Model-View-Controller (MVC) Explained', 'author': 'Techopedia'},\n",
       " {'title': 'Python Game Development: Creating a Snake Game',\n",
       "  'author': 'Real Python'},\n",
       " {'title': 'Paper A', 'author': 'Author A'},\n",
       " {'title': 'Paper B', 'author': 'Author B'},\n",
       " {'title': 'Paper C', 'author': 'Author C'},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
