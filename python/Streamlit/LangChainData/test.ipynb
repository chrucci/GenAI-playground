{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import jsonlines\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "from llama import BasicModelRunner\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.47k/7.47k [00:00<00:00, 22.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned dataset:\n",
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'}\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n"
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrate prompts (add data to prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for row in top_m:\n",
    "  if not row[\"input\"]:\n",
    "    processed_prompt = prompt_template_without_input.format(instruction=row[\"instruction\"])\n",
    "  else:\n",
    "    processed_prompt = prompt_template_with_input.format(instruction=row[\"instruction\"], input=row[\"input\"])\n",
    "\n",
    "  processed_data.append({\"input\": processed_prompt, \"output\": row[\"output\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Below is an instruction that describes a task. Write a response '\n",
      "          'that appropriately completes the request.\\n'\n",
      "          '\\n'\n",
      "          '### Instruction:\\n'\n",
      "          'Give three tips for staying healthy.\\n'\n",
      "          '\\n'\n",
      "          '### Response:',\n",
      " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '\n",
      "           'and vegetables. \\n'\n",
      "           '2. Exercise regularly to keep your body active and strong. \\n'\n",
      "           '3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 388/388 [00:00<00:00, 2.01MB/s]\n",
      "Downloading data: 100%|██████████| 12.7M/12.7M [00:00<00:00, 16.3MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1721.09it/s]\n",
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 901711.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path_hf = \"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf)\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not instruction-tuned output (Llama 2 Base): .\n",
      "Tell me how to train my dog to sit. I have a 10 month old puppy and I want to train him to sit. I have tried the treat method and he just sits there and looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks\n"
     ]
    }
   ],
   "source": [
    "non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
    "non_instruct_output = non_instruct_model(\"Tell me how to train my dog to sit\")\n",
    "print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned output (Llama 2):  on command.\n",
      "How to Train Your Dog to Sit on Command\n",
      "Training your dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:\n",
      "1. Choose a Quiet and Distraction-Free Area: Find a quiet area with minimal distractions where your dog can focus on you.\n",
      "2. Have Treats Ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
      "3. Stand in Front of Your Dog: Stand in front of your dog and hold a treat close to their nose.\n",
      "4. Move the Treat Above Your Dog's Head: Slowly move the treat above your dog's head, towards their tail. As your dog follows the treat with their nose, their bottom will naturally lower into a sitting position.\n",
      "5. Say \"Sit\" and Reward: As soon as your dog's butt touches the ground, say \"Sit\" and give them the treat. It's important to say the command word as they're performing\n"
     ]
    }
   ],
   "source": [
    "instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "instruct_output = instruct_model(\"Tell me how to train my dog to sit\")\n",
    "print(\"Instruction-tuned output (Llama 2): \", instruct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Currently this user has support for base models: ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/util/run_ai.py:134\u001b[0m, in \u001b[0;36mpowerml_send_query_to_url\u001b[0;34m(params, route)\u001b[0m\n\u001b[1;32m    131\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    132\u001b[0m         url\u001b[39m=\u001b[39murl \u001b[39m+\u001b[39m route, headers\u001b[39m=\u001b[39mheaders, json\u001b[39m=\u001b[39mparams, timeout\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.powerml.co/v1/llama/run_program",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chris/Dev/LangChainData/test.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/chris/Dev/LangChainData/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m chatgpt \u001b[39m=\u001b[39m BasicModelRunner(\u001b[39m\"\u001b[39m\u001b[39mchat-gpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/chris/Dev/LangChainData/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m instruct_output_chatgpt \u001b[39m=\u001b[39m chatgpt(\u001b[39m\"\u001b[39;49m\u001b[39mTell me how to train my dog to sit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/chris/Dev/LangChainData/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInstruction-tuned output (ChatGPT): \u001b[39m\u001b[39m\"\u001b[39m, instruct_output_chatgpt)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/runners/basic_model_runner.py:51\u001b[0m, in \u001b[0;36mBasicModelRunner.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# Singleton\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     input_objects \u001b[39m=\u001b[39m Input(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39minputs)\n\u001b[0;32m---> 51\u001b[0m output_objects \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm(\n\u001b[1;32m     52\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49minput_objects,\n\u001b[1;32m     53\u001b[0m     output_type\u001b[39m=\u001b[39;49mOutput,\n\u001b[1;32m     54\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     55\u001b[0m     enable_peft\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_peft,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output_objects, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     58\u001b[0m     outputs \u001b[39m=\u001b[39m [o\u001b[39m.\u001b[39moutput \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m output_objects]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/builder.py:89\u001b[0m, in \u001b[0;36mBuilder.__call__\u001b[0;34m(self, input, output_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_model(\u001b[39minput\u001b[39m, output_type, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m     result \u001b[39m=\u001b[39m gen_value(value)\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/util/api_actions.py:205\u001b[0m, in \u001b[0;36mgen_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen_value\u001b[39m(value: Value):\n\u001b[0;32m--> 205\u001b[0m     value\u001b[39m.\u001b[39;49m_compute_value()\n\u001b[1;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39m_data\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/value.py:65\u001b[0m, in \u001b[0;36mValue._compute_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     params \u001b[39m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprogram\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mto_dict(),\n\u001b[1;32m     63\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrequested_values\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index],\n\u001b[1;32m     64\u001b[0m     }\n\u001b[0;32m---> 65\u001b[0m     response \u001b[39m=\u001b[39m query_run_program(params)\n\u001b[1;32m     67\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m     69\u001b[0m     \u001b[39m# update the cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/util/run_ai.py:11\u001b[0m, in \u001b[0;36mquery_run_program\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery_run_program\u001b[39m(params):\n\u001b[0;32m---> 11\u001b[0m     resp \u001b[39m=\u001b[39m powerml_send_query_to_url(params, \u001b[39m\"\u001b[39;49m\u001b[39m/v1/llama/run_program\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/llama/program/util/run_ai.py:167\u001b[0m, in \u001b[0;36mpowerml_send_query_to_url\u001b[0;34m(params, route)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m         json_response \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mraise\u001b[39;00m llama\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mUserError(json_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdetail\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUserError\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m503\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mUserError\u001b[0m: Currently this user has support for base models: ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf']"
     ]
    }
   ],
   "source": [
    "chatgpt = BasicModelRunner(\"chat-gpt\")\n",
    "instruct_output_chatgpt = chatgpt(\"Tell me how to train my dog to sit\")\n",
    "print(\"Instruction-tuned output (ChatGPT): \", instruct_output_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 396/396 [00:00<00:00, 1.13MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 14.2MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 288kB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/chris/Dev/LangChainData/test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/chris/Dev/LangChainData/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mEleutherAI/pythia-70m\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/chris/Dev/LangChainData/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mEleutherAI/pythia-70m\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/transformers/utils/import_utils.py:1124\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1124\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai-dev/lib/python3.11/site-packages/transformers/utils/import_utils.py:1112\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1110\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
